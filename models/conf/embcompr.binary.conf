#
#   train binary models
#   see embcompr.example.conf for explanations
#


# ------------------------------------------------------------

[256x2]
  [[dimensions]]
  components = 256
  codelength = 2


[defaults]

  [[general]]
  epochs = 2000
  batch_size = 128

  [[embedding]]
  provider = h5py
  file_name = opt/embeddings/fasttext/wiki.de.400k.h5
  vocabulary = opt/embeddings/fasttext/wiki.de.400k.vocabulary.pickle

  [[dimensions]]
  embedding = 300

  [[parameters]]
  gumbel scale = 1
  gumbel temperature = 1.25, 1, 'lambda x, e: x - 1 / 2000'
  learningrate = 0.0005
  momentum = 0.9

  [[statistics]]
  folder = opt/experiments/current
  losses = 1
  codes = 100
  model = 0, 500, 1000, 1250, 1500, 1750, 2000
  encoder gradients = 0
  decoder gradients = 0
