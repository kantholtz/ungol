#
#    EMBCOMPR CONFIGURATION FILE
#    used for ungol/embcompr.py
#    requires configobj
#
#    At least two sections must be provided:
#
#    - [defaults] describing all parameters that are not
#      configured by experiment sections
#
#    - [EXP]+ arbitrarily many sections describing
#      the experiment configuration. The section's name
#      is also used as the target name for the
#      [statistics][folder] subfolder to write all stats to.
#

# ------------------------------------------------------------

[experiment1]
  # ...

[experiment2]
  # ...


# ------------------------------------------------------------

[defaults]

  #
  #  GENERAL CONFIGURATION
  #  ----------------------------------------
  [[general]]
  epochs = 1000

  # size of batches fed to the model
  batch size = 128
  # size of the data loaded to memory: data is fetched from disk to
  # ram and then from ram to the graphics card - this assumes the
  # common case where the machine has much more RAM then the graphics
  # card. Usually the ram chunk size should be greater than all
  # samples because loading from disk is very slow...  This was
  # implemented to work with very large datasets. To load all samples
  # to the GPU once, simply set both to a value greater than the
  # number of samples.
  ram chunk size = 1000000
  gpu chunk size = 200000
  # pre-sampled gumbel noise size
  noise buffer = 268435456  # 2**28 (1G)
  # noise buffer = 536870912  # 2**29 (2G)
  # noise buffer = 1073741824  # 2**30 (4G)
  # noise buffer = 2147483648  # 2**31 (8G)


  #
  #  EMBEDDING CONFIGURATION
  #  ----------------------------------------
  [[embedding]]
  # for supported providers see (or extend) ungol.common.embed

  # provider = file
  # file_name = opt/embeddings/fasttext/wiki.de.400k.vec

  provider = h5py
  file name = opt/embeddings/fasttext/wiki.de.400k.h5
  vocabulary = opt/embeddings/fasttext/wiki.de.400k.vocabulary.pickle

  #
  #  MODEL CONFIGURATION
  #  ----------------------------------------
  [[dimensions]]
  components = 16  # M
  codelength = 16  # K

  # output neurons of the first hidden layer
  # defaults to (components * codelength // 2)
  # fcl1 = 2000


  #
  #  HYPERPARAMETERS
  #  ----------------------------------------
  [[parameters]]
  # most parameters can either be fixed floating point values
  # or be hardened/softened throughout training:
  # the format is the following:
  #    initial, step, fn
  # where fn is a function which describes how the latest parameter
  # value is updated. The function fn is called every <step> epoch.
  # The provided parameter <x> is the lates value
  #    0.1, 10, lambda x: x / 2
  # means to halve the initial 0.1 every 10 epochs.
  # ----------------------------------------
  # scale of sampled values
  # see: https://pytorch.org/docs/stable/distributions.html#torch.distributions.gumbel.Gumble
  # the scaling factor is any real number including 0
  gumbel scale = 1
  # gumbel temperature, applied to the softmax operands;
  # it must not be zero (it is used as divisor)
  gumbel temperature = 1, 1, 'lambda x, e: x - 0.5/2000'
  # only initial for adaptive optimizers
  learningrate = 0.001
  # only applied to some optimizers
  momentum = 0.9
  # observes the delta between the mean of a window
  # and the last half of the window and ends training
  # if the threshold is undercut
  early stopping = 100, 0.05

  # INITIALIZATION
  # possible initialization methods:
  # [!] Note the comma after single value options!
  encoder init = uniform, -1, 1  # uniform, lower, upper
  # encoder init = normal, 0, 1     # normal, mean, stddev
  # encoder init = xavier uniform,  # 'gain' is calculated based on activation functions
  # encoder init = xavier normal,   # 'gain' is calculated based on activation functions
  # encoder init = embcompr, 'opt/models/compressor.torch'

  # Codebooks are initialized by randomly sampling from the training
  # data. Sometimes the norms of these vectors are too large and training
  # dies. Thus this factor is multiplied with the initialization.
  decoder init factor = 1e-3

  #
  #   STATISTICS
  #   ----------------------------------------
  #
  #   for details see inline comments of embcompr._Statistics
  #   values must be >= 0 (deactivated if 0) or a list containing
  #   the relevant epochs.
  #
  [[statistics]]
  # folder to write aggregated data to
  folder = opt/experiments/current
  # aggregate loss values
  losses = 1
  # aggregate gradient values
  encoder gradients = 0  # currently unused
  decoder gradients = 0  # currently unused
  # aggregate post-softmax activations of the encoder
  codes = 100
  # save model parameters
  model = 800, 1000
