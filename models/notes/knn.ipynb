{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Exploring knn exemplary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import ungol.models.analyze as uma\n",
    "\n",
    "import attr\n",
    "import h5py\n",
    "from tabulate import tabulate\n",
    "\n",
    "import re\n",
    "import pickle\n",
    "import random\n",
    "import pathlib\n",
    "from collections import defaultdict\n",
    "\n",
    "from typing import List\n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "#  this section applies some heuristics to identify\n",
    "#  distance mappings automatically\n",
    "#\n",
    "def _compile(s: str):\n",
    "    return re.compile(f'.*\\W{s}\\W.*', )\n",
    "\n",
    "def _ref(*ls: List[str]):\n",
    "    return [(s, _compile(s)) for s in ls]\n",
    "\n",
    "REF_REDUX = _ref('bow', 'mbow', 'sif', 'sent2vec', )\n",
    "REF_KIND = _ref('euclidean', 'cosine', 'hamming', )\n",
    "REF_DATASET = _ref('sick', 'sts', 'sicksts', 'enwiki_?\\w*')\n",
    "REF_RECON = _ref('recon\\.model')\n",
    "\n",
    "\n",
    "class DistmapException(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "@attr.s\n",
    "class Distmap:\n",
    "\n",
    "    path:    pathlib.Path = attr.ib()\n",
    "    vocab: Dict[str, int] = attr.ib()\n",
    "\n",
    "    name:     str = attr.ib()\n",
    "    dataset:  str = attr.ib()\n",
    "    redux:    str = attr.ib()\n",
    "    kind:     str = attr.ib()\n",
    "    amount:   int = attr.ib()\n",
    "    recon:   bool = attr.ib()\n",
    "\n",
    "    @property\n",
    "    def nn(self) -> uma.Neighbours:\n",
    "        return self._nn\n",
    "\n",
    "    @property\n",
    "    def row(self):\n",
    "        return (\n",
    "            self.name,\n",
    "            self.kind,\n",
    "            self.redux,\n",
    "            self.dataset,\n",
    "            self.recon,\n",
    "            self.amount,\n",
    "            self.path.name,\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def headers(self):\n",
    "        return (\n",
    "            'name',\n",
    "            'kind',\n",
    "            'redux',\n",
    "            'dataset',\n",
    "            'recon',\n",
    "            'amount',\n",
    "            'file',\n",
    "        )\n",
    "\n",
    "    def __attrs_post_init__(self):\n",
    "        self._nn = uma.Neighbours.from_file(self.path, self.vocab)\n",
    "        assert len(self._nn.vocabulary) == self.amount\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        s_buf = [f'Distmap \"{self.name}\"']\n",
    "        s_buf.append(f'  {self.dataset}.{self.redux}-{self.kind}')\n",
    "        s_buf.append(f'  {self.amount} samples (recon={self.recon})')\n",
    "        return '\\n'.join(s_buf)\n",
    "\n",
    "    @staticmethod\n",
    "    def from_path(name: str, p: pathlib.Path, vocab):\n",
    "        with h5py.File(p, mode='r') as fd:\n",
    "            amount = fd['dists'].shape[0]\n",
    "\n",
    "        def identify(ref, unique: bool = True):\n",
    "            matches = [name for (name, r) in ref if r.match(str(p))]\n",
    "\n",
    "            if unique and len(matches) == 1:\n",
    "                return matches[0]\n",
    "            elif unique:\n",
    "                raise DistmapException()\n",
    "\n",
    "            return matches\n",
    "\n",
    "        return Distmap(\n",
    "            name=name,\n",
    "            path=p,\n",
    "            dataset=identify(REF_DATASET),\n",
    "            redux=identify(REF_REDUX),\n",
    "            kind=identify(REF_KIND),\n",
    "            recon=len(identify(REF_RECON, unique=False)) > 0,\n",
    "            amount=amount,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def from_codes_dir(name: str, p: pathlib.Path, vocabs):\n",
    "        \"\"\"\n",
    "        This method reads distmaps from opt/codes. They must follow\n",
    "        the following naming convention:\n",
    "\n",
    "          opt/codes/<EVAL_DATASET>/<TRAIN_MODEL>.hamming-dist.h5\n",
    "\n",
    "        where <TRAIN_MODEL> has the following format:\n",
    "\n",
    "          <DATASET>.<REDUX>-<BITS>.model-<EPOCH>\n",
    "\n",
    "        \"\"\"\n",
    "        with h5py.File(p, mode='r') as fd:\n",
    "            amount = fd['dists'].shape[0]\n",
    "\n",
    "        dataset, redux, bits, _, model, _, _, _ = re.split(r'[.-]', p.name)\n",
    "        assert redux in vocabs\n",
    "\n",
    "        return Distmap(\n",
    "            name=name,\n",
    "            path=p,\n",
    "            vocab=vocabs[redux],\n",
    "            dataset=dataset,\n",
    "            redux=redux,\n",
    "            kind='hamming',\n",
    "            recon=False,\n",
    "            amount=amount, )\n",
    "\n",
    "\n",
    "def find_distmaps(p_root: pathlib.Path):\n",
    "    \"\"\"\n",
    "    Apply some heuristics to automatically identify distance mappings\n",
    "    \"\"\"\n",
    "    distmaps = []\n",
    "\n",
    "    for did, glob in enumerate(p_root.glob('**/*dist*h5')):\n",
    "        try:\n",
    "            distmaps.append(Distmap.from_path(f'dm-{did}', glob))\n",
    "        except DistmapException:\n",
    "            print(f'could not convert {glob}')\n",
    "\n",
    "    assert len(distmaps) > 0\n",
    "    return distmaps\n",
    "\n",
    "\n",
    "def find_code_distmaps(p_root: pathlib.Path, vocabs):\n",
    "    distmaps = []\n",
    "    for did, glob in enumerate(p_root.glob('**/*hamming-dist.h5')):\n",
    "        try:\n",
    "            dm = Distmap.from_codes_dir(f'dm-{did}', glob, vocabs)\n",
    "            distmaps.append(dm)\n",
    "        except DistmapException:\n",
    "            print(f'could not convert {glob}')\n",
    "\n",
    "    assert len(distmaps)\n",
    "    return distmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# distmaps = find_distmaps(pathlib.Path('../opt/experiments'))\n",
    "# rows = sorted([dm.row for dm in distmaps], key=lambda l: l[3])\n",
    "# print(tabulate(rows, headers=distmaps[0].headers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "DATASET = 'sick'\n",
    "\n",
    "def load_codes_distmaps(dataset: str):\n",
    "    print('\\nloading vocabularies')\n",
    "    vocabs = {}\n",
    "    for glob in pathlib.Path(f'../opt/data/{DATASET}').glob('*.vocab.pickle'):\n",
    "        redux, _ = glob.stem.split('.')\n",
    "        print(f'loading {glob}: redux={redux}')\n",
    "        vocabs[redux] = str(glob)\n",
    "\n",
    "    distmaps = find_code_distmaps(pathlib.Path('../opt/codes/sick'), vocabs)\n",
    "    rows = [dm.row for dm in distmaps]\n",
    "    print(tabulate(rows, headers=distmaps[0].headers))\n",
    "\n",
    "    return distmaps, vocabs\n",
    "\n",
    "distmaps, vocabs = load_codes_distmaps(DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "k = 5\n",
    "\n",
    "with open(vocabs['mbow'], mode='rb') as fd:\n",
    "    _vocab = pickle.load(fd)\n",
    "    _sentences = list(_vocab.keys())\n",
    "    random.shuffle(_sentences)\n",
    "    sample = _sentences[0]\n",
    "\n",
    "for dm in distmaps:\n",
    "    print('-' * 80)\n",
    "    print(f'\\n{dm}\\n')\n",
    "    print(f\"Neighbours of {sample}\")\n",
    "    for word, dist in [(n.word, n.dist) for n in dm.nn[sample][:k]]:\n",
    "        print(f'  {dist:7.3f} | {word}')\n",
    "\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "---------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "assert False, 'legacy code below.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import ungol.common.embed as uce\n",
    "import ungol.models.analyze as uma\n",
    "\n",
    "import torch\n",
    "from tabulate import tabulate\n",
    "\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Search Nearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "dataset = '../opt/bow/sick'\n",
    "\n",
    "f_redux1 = 'mbow'\n",
    "f_embed1 = f'{dataset}/{f_redux1}.embedding.h5'\n",
    "f_vocab1 = f'{dataset}/{f_redux1}.vocab.pickle'\n",
    "f_dists1 = f'{dataset}/{f_redux1}.cosine-dist.h5'\n",
    "\n",
    "nn1 = uma.Neighbours.from_file(f_dists1, f_vocab1)\n",
    "e1 = uce.create(uce.Config(provider='h5py', file_name=f_embed1, vocabulary=f_vocab1, ))\n",
    "\n",
    "f_redux2 = 'sent2vec'\n",
    "f_embed2 = f'{dataset}/{f_redux2}.embedding.h5'\n",
    "f_vocab2 = f'{dataset}/{f_redux2}.vocab.pickle'\n",
    "f_dists2 = f'{dataset}/{f_redux2}.cosine-dist.h5'\n",
    "\n",
    "nn2 = uma.Neighbours.from_file(f_dists2, f_vocab2)\n",
    "e2 = uce.create(uce.Config(provider='h5py', file_name=f_embed2, vocabulary=f_vocab2, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def _print_knn(name, a, k):\n",
    "    print(f'\\n{name}\\n')\n",
    "    for i, neighbour in enumerate(a[:k]):\n",
    "        print(f'  {i}: {neighbour.dist:.3f} {neighbour.word}')\n",
    "\n",
    "selection = list(nn1.vocabulary.keys())[0:1]\n",
    "for sentence in selection:\n",
    "    print('\\n', '-' * 60)\n",
    "    print(f'>> [ {sentence} ] <<')\n",
    "\n",
    "    k = 20\n",
    "\n",
    "    _print_knn(f_redux1, nn1[sentence], k)\n",
    "    _print_knn(f_redux2, nn2[sentence], k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "s1 = 'a man is playing the guitar'\n",
    "\n",
    "sents = [\n",
    "    'a man is playing the guitar',\n",
    "    'a guitar is being played by the man',\n",
    "    'a guitar is being played by a man',\n",
    "    'the baby is not laughing and crawling',\n",
    "    'the back of a small black dog is being sniffed by the brown dog',\n",
    "    'there is no girl jumping into the car', ]\n",
    "\n",
    "cos = torch.nn.functional.cosine_similarity\n",
    "\n",
    "nn_lis1 = [n.word for n in nn1[s1]]\n",
    "nn_lis2 = [n.word for n in nn2[s1]]\n",
    "\n",
    "def print_cos(e: uce.Embed, nn_lis: List[str]):\n",
    "    print(f'\\n\"{s1}\"\\n')\n",
    "    ref_e = torch.from_numpy(e[s1])\n",
    "\n",
    "    for s in sents:\n",
    "        sim = cos(ref_e, torch.from_numpy(e[s]), dim=0)\n",
    "        pos = nn_lis.index(s) if s in nn_lis else -1\n",
    "\n",
    "        print(f'  {sim:.3f} [{pos:4d}] \"{s}\"')\n",
    "\n",
    "print_cos(e1, nn_lis1)\n",
    "print_cos(e2, nn_lis2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "nncos = ua.Neighbours.from_file('../opt/neighbours/glove-cosine.h5', f_vocab)\n",
    "nnham = ua.Neighbours.from_file('../opt/experiments/binary/glove-256x2/hamming-dists.h5', f_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def _print_disjunkt_knn(word, ref, cmp):\n",
    "    ref_words, ref_nn = ref\n",
    "    cmp_words, cmp_nn = cmp\n",
    "\n",
    "    neighbours = cmp_nn[word]\n",
    "\n",
    "    ref_name = ref_nn.fd.filename.split('/')[-1]\n",
    "    cmp_name = cmp_nn.fd.filename.split('/')[-1]\n",
    "\n",
    "    print('\\n\\nwords of {} (reference) not in found in {} (compare)\\n'.format(ref_name, cmp_name))\n",
    "    data = []\n",
    "\n",
    "    # no optimal runtime complexity but data is small at this point.\n",
    "    for missing_word in [w for w in ref_words if w not in cmp_words]:\n",
    "        for i, nn in enumerate(neighbours):\n",
    "            if nn.word == missing_word:\n",
    "                data.append((missing_word, i, nn.dist))\n",
    "\n",
    "    print(tabulate(data, headers=('word', 'compare idx', 'compare dist', )))\n",
    "\n",
    "def print_knn(word, k=10):\n",
    "    c_words, c_dists = zip(*[(n.word, n.dist) for n in nncos[word][1:k+1]])\n",
    "    h_words, h_dists = zip(*[(n.word, n.dist) for n in nnham[word][1:k+1]])\n",
    "\n",
    "    nn = zip(range(1, k+1), c_words, c_dists, h_words, h_dists)\n",
    "    headers = \"idx\", \"cosine word\", \"cosine dist\", \"hamming word\", \"hamming dist\"\n",
    "\n",
    "    print(tabulate(nn, headers=headers))\n",
    "    _print_disjunkt_knn(word, (c_words, nncos), (h_words, nnham))\n",
    "    _print_disjunkt_knn(word, (h_words, nnham), (c_words, nncos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def print_summary(word: str):\n",
    "    print('-' * 80, '\\n', word.upper(), '\\n')\n",
    "    print_knn(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print_summary('compressor')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "name": "knn.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
