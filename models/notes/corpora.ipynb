{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dumpr.common as dc\n",
    "\n",
    "import nltk\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import pickle\n",
    "import pathlib\n",
    "import multiprocessing as mp\n",
    "from collections import defaultdict\n",
    "\n",
    "from typing import Tuple\n",
    "from typing import Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLEF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "basepath = pathlib.Path('/mnt/maehre/dump/text/de/')\n",
    "files = [\n",
    "    'frankfurter-rundschau-9495/frankfurter-rundschau-9495.full.xml',\n",
    "    'sda-94/sda-94.full.xml',\n",
    "    'sda-95/sda-95.full.xml',\n",
    "    'spiegel-9495/spiegel-9495.full.xml', ]\n",
    "\n",
    "\n",
    "def gen_docs() -> Generator[str, None, None]:\n",
    "    for xml in (basepath/f for f in files):\n",
    "        with dc.BatchReader(str(xml)) as reader:\n",
    "            for doc in reader:\n",
    "                yield doc.content\n",
    "\n",
    "\n",
    "def process(content: str) -> Tuple[Tuple[str, int]]:\n",
    "    vocab = defaultdict(lambda: 0)\n",
    "    for w in nltk.word_tokenize(content):\n",
    "        vocab[w.lower()] += 1\n",
    "    \n",
    "    return tuple(vocab.items())\n",
    "     \n",
    "\n",
    "def run():\n",
    "    total = 13781 + 69438 + 71677 + 139715\n",
    "    \n",
    "    vocab = defaultdict(lambda: 0)\n",
    "    merge_bar = tqdm(total=total, position=1, desc='merged')\n",
    "    \n",
    "    def merge(result):    \n",
    "        for word, count in result:\n",
    "            vocab[word] += count\n",
    "        merge_bar.update(1)\n",
    "        \n",
    "    with mp.Pool(3) as pool:\n",
    "        results = []\n",
    "        \n",
    "        for i, content in tqdm(enumerate(gen_docs()), total=total, desc='read'):\n",
    "            res = pool.apply_async(process, (content, ), callback=merge)\n",
    "            results.append(res)\n",
    "\n",
    "        for res in results:\n",
    "            res.wait()\n",
    "    \n",
    "    mapping = [(k, v) for v, k in sorted({v: k for k, v in vocab.items()}.items(), reverse=True)]\n",
    "    \n",
    "    print('writing text file')\n",
    "    with open('../opt/clef.vocab.txt', mode='w') as fd:\n",
    "        for tup in mapping:\n",
    "            fd.write('{} {}\\n'.format(*tup))\n",
    "    \n",
    "    print('dumping dict')\n",
    "    with open('../opt/clef.vocab.pickle', mode='wb') as fd:\n",
    "        pickle.dump(dict(mapping), fd)\n",
    "                               \n",
    "    merge_bar.close()\n",
    "\n",
    "run()\n",
    "print('\\ndone.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../opt/clef.vocab.pickle', mode='rb') as fd:\n",
    "    vocab = pickle.load(fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('vocabulary size', len(vocab))\n",
    "print('total word count', sum(vocab.values()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ungol-models",
   "language": "python",
   "name": "ungol-models"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
